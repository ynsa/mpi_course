// ... first minutes
// Баханович Сергей Викторович
/*
	Коммуникаторы с топологией
*/

int MPI_Cart_create(
	MPI_Comm comm,
	int ndims,
	int *dims,
	int *periods,
	int reorder,
	MPI_Comm *cart_comm
);
/* Стоит позаботиться о том, чтобы построенная решётка адекватно отразилась на архитектурную систему. 
	Не всегда система справляется с этим хорошо(а мы ей говорим это делать при флаге reorder)
	
	В чём польза декартовой решётки? Каждый процесс имеет координаты.
	Каждый процесс имеет теперь не только ранг, но и коорлинаты.
*/

// по № процесса определяем координаты
int MPI_Cart_coords(
	MPI_Comm comm, 
	int rank,
	int ndims, // размер решётки
	int* coords
);

// по координатам определяем № процесса
int MPI_Cart_rank(
	MPI_Comm comm, 
	int* coords,
	int* rank
);

/* № процессов могут раздаться как угодно, поэтому можно полагаться лишь на ранги и координаты, а не на номера.*/

/* Фрагментация по строкам - нарезаем весь коммуникатор на подкоммуникаторы*/


/*
	i_2 ^
		|
		|
		----->
			i_1
*/
int MPI_Cart_sub(
	MPI_Comm cartcomm,
	int* remaindims,
	MPI_Comm* subcomm
);

/*
	В направлении i_1: remaindims = {1, 0}, в направлении i_2 = {0, 1}
*/

/* Для того, чтоб воспользоваться ф-ей обмена, надо зафиксировать кооринаты, получить ранг, то же самое сделать для destination и только потом использовать ф-ию обмена.
	Для всего этого можно воспользоваться ф-ей:
*/
int MPI_Cartshift(
	MPI_Comm cartcomm,
	int direction, // направление смещения (1, -1) : 1 - в rank_source процесс с наименьшим номеромб -1 - в rank_source процесс с наибольшим номером
	int disp, // если интересует соседний, смещаемся на 1, если интересует через одни -> 2
	int* rank_source, 
	int* rank_dist 
);
/* по rank_source и rank_dist одновременно получаем ранги необходимых процессов.
	Если по решётке требуемого соседа нет, то возвращается MPI_Proc_NULL(если не указана длинная связь при построении коммуникатора)`
*/

/* Note: 
	Ф-ия создания коммуникатора - коллективная -> при создании коммуникатора можем получить неявную барьерную синхронизацию. 
	Стандарт предлагает копирование коммуникатора сделать локально, однако это тоже не идеально.
*/



/* КОЛЛЕКТИВНЫЕ ФУНКЦИИ ОБМЕНА*/

/* В коллективных ф-иях нет тегов, поэтому необходимо в одинаковой последовательности выхывать такие ф-ии. В ф-их типа точка-точка несоответствие порядка допускалось.
	Операции обмена в коллективных ф-иях не пересекаются ни между собой, ни с обменами типа точка-точка.*/

int MPI_Barrier(
	MPI_Comm comm
);

/* блокирующая ф-ия. Останавливает процесс, пока все остальные процессы коммуникатора не вызовут эту ф-ию.
	В хорошей программе этой ф-ии должно быть как можно меньеш( по очевидным причинам)
*/


int MPI_Bcast(
	void* buf,
	int count,
	MPI_Datatype type,
	int root, // кто отсылает
	MPI_Comm comm
);

/* Ф-ия отпарвляет данные всем процессам. Правила укладки полученных данныхмогут быть уникальны в каждом процессе.*/

int MPI_Gather(
	void* sbuf,
	int scount,
	MPI_Datatype stype,
	void* rbuf,
	int rcount,
	MPI_Datatype rtype,
	int root,
	MPI_Comm comm
);

/* Ф-ия позволяет собрать данные с разных процессов на одном. Укладываются по № процесса. Первая тройка одинкова для всех процессов. Следующая тройка актуальноатолько для корневого процесса.
	Правило укладки, указанное в ф-ии(строка 104-105), указано только для фрагмента! -> Оно едино для всех полученных фрагментов. Сами фрагменты могут изыматься как угодно, уникально
	для каждого отдельного процесса.
*/

int MPI_Scatter(
	void* sbuf,
	int scount, // склько раз применяем выборку
	// здесь так же может быть массив фрагментов + массив смещений для особенных типов
	MPI_Datatype stype, // как выбираем
	void* rbuf,
	int rcount,
	MPI_Datatype rtype,
	int root,
	MPI_Comm comm
);

/* Обратная к MPI_Gather: из одного процесса во многие. Первая тройка - спецификация буффера, из которого данные распределяем на корневом процессе 
	(второй и третий параметр определяет правило выбора 1-ого фрагмента)
*/

// 3...
// 2...
// 1...
// Пример!

int a[N][N];
int p; // N / p - количество столбцов
int b[N / p][N]; // b[N / p * N]
if (myrank == 0){
	MPI_Type_vector(N, 1, N, MPI_INT, &coltype);
	// переопределяем extent:
	MPI_Type_create_resized(
		coltype, 0, sizeof(int), mcoltype
	);
	// регистрируем новый тип:
	MPI_Type_commit(&mcoltype);
	// вызываем ф-ию распределения данных
	MPI_Scatter(a, N / p, mcoltype, b, N / p * N, MPI_INT, 0, comm);
}
else{
	MPI_Scatter(
		NULL, 0, MPI_INT, //сюда можно писать хоть вашулюбимую песню
		b, N / p * N, MPI_INT, 0, comm
	);
}


int MPI_Gaterall
int MPI_Gathercallv
int MPI_Alltoall(...);
/* MPI_Alltoall - каждый процесс фрагментирует свой массив на p фрагментов, затем отдельный фрагмент отсылается отдельному процессу.*/

int MPI_Reduce(
	void* buf,
	void* res,
	int count,
	MPI_Datatype type,
	MPI_Op op, // указываем код операции, можно создать свою
	int root,
	MPI_Comm comm
);

/* Все операции выполняются покоординатно. */


/* АНАЛИЗ ПОДХОДОВ К ПЕРЕМНОЖЕНИЮ МАТРИЦ */

/*
1. Матрица А нарезается по строкам, В целиком у каждого.
2.
3. Ленточная: матрица А по строкам, В - по столбцам, С - диагональный блоки на стартовом распределении данных. Далее мы берём матрицу В и делаем циклический сдвиг по кольцу процессов
	для получения диагонали выше(или ниже) основной оставшихся блоков матрицы С. Способ удобен, если В не помещается в память столько раз.
	Оценка времени:
		T_comp = O(N^3 / p^2) // computation
		T_comm = O(N^2 / p) = a + b * N^2 / p // communcation, b - пропускная способность; a -  латентность. Все обменыравносильны одному по времени

		T = O(p * N^3 / p^2) + O(N^2) = O(N^3 / p) + O(N^2)
		-> алгоритм неплох на небольшом количестве процессов, при большом второе слагаемое(а оно как бы от p не зависит, но на самом деле увеличивается латентность
		при увеличении количества процессов) соу... всё будет плохо.
		
	Ленточный алгоритм идеален для процессов, объединённых в кольцо.
4. Алгоритм Фокса: p = q^2. Нарезаем матрица А и В на блоки, всего q блоков. Внимание! Людям с аллергией на коорлинаты, просьба не читать последующий контент.

	И тут мы используем коммуникатор с виртуальной декартовой топологией.

	for(step = 0; step < q; step++){
		// рассылаем A_11 всем процессам 1-ой строки
		Fix & Bcast(A_ik), where k = (i + step) % q; in row i
		Mult(A_ik, B_ij)
		// циклический сдвиг по столбцу в сторону меньшего номера(Т.Е. вверх по матричке)
		Shift(B_i1)
	}

	Оценка времени:
		T_comp = O(N^3 / q^3)
		T_comm = O(N^2 / q^2) + O((N^2 / q^2) * log_2q(q))// оценка циклического сдвига на одном шаге

		T = O(N^3 / p) + O(N^2 / q) + O((N^2 / q) * log_2q(q))


*/
